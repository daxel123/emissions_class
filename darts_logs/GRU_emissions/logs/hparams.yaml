activation: ReLU
dropout: 0.2
hidden_dim: 64
input_chunk_length: 12
input_size: 8
likelihood: null
lr_scheduler_cls: null
lr_scheduler_kwargs: null
name: GRU
nr_params: 1
num_layers: 2
num_layers_out_fc: []
optimizer_cls: !!python/name:torch.optim.adam.Adam ''
optimizer_kwargs:
  lr: 0.001
output_chunk_length: 6
output_chunk_shift: 0
target_size: 1
train_sample_shape:
- !!python/tuple
  - 12
  - 1
- !!python/tuple
  - 12
  - 7
- null
- !!python/tuple
  - 6
  - 1
use_reversible_instance_norm: false
